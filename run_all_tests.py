import os
import subprocess
import sys
import time
from pathlib import Path
import argparse
import json
import numpy as np
import matplotlib.pyplot as plt
import re

# è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•å¹¶è®¾ç½®ä¸ºå·¥ä½œç›®å½•
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
os.chdir(CURRENT_DIR)
print(f"Current working directory set to: {CURRENT_DIR}")

def find_all_patients(dataset_dir):
    """æ‰¾åˆ°æ‰€æœ‰patientçš„æ–‡ä»¶å¯¹"""
    patient_pairs = []
    train_dir = os.path.join(dataset_dir, 'train')
    
    if not os.path.exists(train_dir):
        print(f"Error: Training directory not found: {train_dir}")
        return patient_pairs
    
    print(f"Scanning for patients in: {train_dir}")
    
    for patient_id in sorted(os.listdir(train_dir)):
        patient_dir = os.path.join(train_dir, patient_id)
        if os.path.isdir(patient_dir):
            fla_path = os.path.join(patient_dir, f"{patient_id}_fla.nii.gz")
            seg_path = os.path.join(patient_dir, f"{patient_id}_seg.nii.gz")
            
            if os.path.exists(fla_path) and os.path.exists(seg_path):
                patient_pairs.append({
                    'patient_id': patient_id,
                    'fla_path': fla_path,
                    'seg_path': seg_path
                })
                print(f"Found patient: {patient_id}")
            else:
                print(f"Warning: Incomplete data for patient {patient_id}")
                if not os.path.exists(fla_path):
                    print(f"  Missing: {fla_path}")
                if not os.path.exists(seg_path):
                    print(f"  Missing: {seg_path}")
    
    print(f"Total patients found: {len(patient_pairs)}")
    return patient_pairs

def run_single_test(patient_info, model_path, output_base_dir, compare_script_path):
    """è¿è¡Œå•ä¸ªpatientçš„æµ‹è¯•"""
    patient_id = patient_info['patient_id']
    fla_path = patient_info['fla_path']
    seg_path = patient_info['seg_path']
    
    # ä¸ºæ¯ä¸ªpatientåˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
    patient_output_dir = os.path.join(output_base_dir, f"patient_{patient_id}")
    
    print(f"\n{'='*60}")
    print(f"Testing Patient: {patient_id}")
    print(f"FLA file: {fla_path}")
    print(f"SEG file: {seg_path}")
    print(f"Output dir: {patient_output_dir}")
    print(f"{'='*60}")
    
    # æ„å»ºå‘½ä»¤
    cmd = [
        sys.executable,  # ä½¿ç”¨å½“å‰Pythonè§£é‡Šå™¨
        compare_script_path,
        "--input_path", fla_path,
        "--ground_truth_path", seg_path,
        "--model_path", model_path,
        "--output_dir", patient_output_dir
    ]
    
    print(f"Running command: {' '.join(cmd)}")
    
    try:
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è¿è¡Œå‘½ä»¤
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        duration = end_time - start_time
        
        if result.returncode == 0:
            print(f"âœ… Patient {patient_id} completed successfully in {duration:.2f} seconds")
            return True, duration, result.stdout
        else:
            print(f"âŒ Patient {patient_id} failed with return code {result.returncode}")
            print(f"Error output: {result.stderr}")
            return False, duration, result.stderr
            
    except subprocess.TimeoutExpired:
        print(f"â° Patient {patient_id} timed out after 5 minutes")
        return False, 300, "Timeout"
    except Exception as e:
        print(f"ğŸ’¥ Patient {patient_id} failed with exception: {e}")
        return False, 0, str(e)

def parse_patient_metrics(patient_output_dir, patient_id):
    """è§£æå•ä¸ªæ‚£è€…çš„æŒ‡æ ‡æ–‡ä»¶"""
    metrics_file = os.path.join(patient_output_dir, f"{patient_id}_metrics.txt")
    
    if not os.path.exists(metrics_file):
        print(f"Warning: Metrics file not found for patient {patient_id}")
        return None
    
    metrics = {}
    try:
        with open(metrics_file, 'r') as f:
            content = f.read()
        
        # è§£ææ•´ä½“æŒ‡æ ‡
        overall_section = content.split("OVERALL METRICS:")[1].split("\n\n")[0]
        for line in overall_section.strip().split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                try:
                    metrics[key.strip()] = float(value.strip())
                except:
                    pass
        
        # è§£æç»Ÿè®¡ä¿¡æ¯
        if "Total slices:" in content:
            total_slices_match = re.search(r"Total slices:\s+(\d+)", content)
            if total_slices_match:
                metrics['total_slices'] = int(total_slices_match.group(1))
        
        if "Slices with tumor:" in content:
            tumor_slices_match = re.search(r"Slices with tumor:\s+(\d+)", content)
            if tumor_slices_match:
                metrics['slices_with_tumor'] = int(tumor_slices_match.group(1))
        
        if "True tumor volume:" in content:
            true_volume_match = re.search(r"True tumor volume:\s+(\d+)", content)
            if true_volume_match:
                metrics['true_tumor_volume'] = int(true_volume_match.group(1))
        
        if "Pred tumor volume:" in content:
            pred_volume_match = re.search(r"Pred tumor volume:\s+(\d+)", content)
            if pred_volume_match:
                metrics['pred_tumor_volume'] = int(pred_volume_match.group(1))
        
        return metrics
        
    except Exception as e:
        print(f"Error parsing metrics for patient {patient_id}: {e}")
        return None

def collect_all_metrics(results, output_base_dir):
    """æ”¶é›†æ‰€æœ‰æ‚£è€…çš„æŒ‡æ ‡"""
    all_metrics = []
    
    for result in results:
        if result['success']:
            patient_id = result['patient_id']
            patient_output_dir = os.path.join(output_base_dir, f"patient_{patient_id}")
            
            metrics = parse_patient_metrics(patient_output_dir, patient_id)
            if metrics:
                metrics['patient_id'] = patient_id
                metrics['duration'] = result['duration']
                all_metrics.append(metrics)
            else:
                print(f"Could not parse metrics for patient {patient_id}")
    
    return all_metrics

def generate_comprehensive_report(results, all_metrics, output_base_dir):
    """ç”Ÿæˆå…¨é¢çš„æµ‹è¯•æŠ¥å‘Š"""
    
    # åŸºæœ¬ç»Ÿè®¡
    total_patients = len(results)
    successful = sum(1 for r in results if r['success'])
    failed = total_patients - successful
    total_time = sum(r['duration'] for r in results)
    
    # è®¡ç®—æŒ‡æ ‡ç»Ÿè®¡
    if all_metrics:
        metric_names = ['dice', 'iou', 'accuracy', 'precision', 'recall', 'f1_score', 'specificity']
        metric_stats = {}
        
        for metric in metric_names:
            values = [m[metric] for m in all_metrics if metric in m]
            if values:
                metric_stats[metric] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'median': np.median(values),
                    'q25': np.percentile(values, 25),
                    'q75': np.percentile(values, 75),
                    'values': values
                }
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report_file = os.path.join(output_base_dir, "comprehensive_test_report.txt")
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("BRAIN TUMOR SEGMENTATION - COMPREHENSIVE TEST REPORT\n")
        f.write("="*80 + "\n")
        f.write(f"Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # æ‰§è¡Œæ¦‚è¦
        f.write("EXECUTION SUMMARY\n")
        f.write("-"*40 + "\n")
        f.write(f"Total patients processed: {total_patients}\n")
        f.write(f"Successful tests: {successful}\n")
        f.write(f"Failed tests: {failed}\n")
        f.write(f"Success rate: {successful/total_patients*100:.1f}%\n")
        f.write(f"Total execution time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)\n")
        f.write(f"Average time per patient: {total_time/total_patients:.2f} seconds\n\n")
        
        if all_metrics:
            # æ•´ä½“æ€§èƒ½ç»Ÿè®¡
            f.write("OVERALL PERFORMANCE STATISTICS\n")
            f.write("-"*40 + "\n")
            f.write(f"Number of patients with valid metrics: {len(all_metrics)}\n\n")
            
            # ä¸»è¦æŒ‡æ ‡ç»Ÿè®¡è¡¨æ ¼
            f.write("KEY METRICS SUMMARY:\n")
            f.write(f"{'Metric':<12} {'Mean':<8} {'Std':<8} {'Min':<8} {'Max':<8} {'Median':<8}\n")
            f.write("-"*60 + "\n")
            
            for metric in ['dice', 'iou', 'accuracy', 'precision', 'recall', 'f1_score']:
                if metric in metric_stats:
                    stats = metric_stats[metric]
                    f.write(f"{metric.upper():<12} {stats['mean']:.4f}   {stats['std']:.4f}   "
                           f"{stats['min']:.4f}   {stats['max']:.4f}   {stats['median']:.4f}\n")
            
            f.write("\n")
            
            # åˆ†çº§è¯„ä¼°
            f.write("PERFORMANCE GRADING\n")
            f.write("-"*20 + "\n")
            if 'dice' in metric_stats:
                dice_mean = metric_stats['dice']['mean']
                if dice_mean >= 0.9:
                    grade = "EXCELLENT (A+)"
                elif dice_mean >= 0.8:
                    grade = "VERY GOOD (A)"
                elif dice_mean >= 0.7:
                    grade = "GOOD (B)"
                elif dice_mean >= 0.6:
                    grade = "FAIR (C)"
                else:
                    grade = "NEEDS IMPROVEMENT (D)"
                
                f.write(f"Overall Model Performance: {grade}\n")
                f.write(f"Average Dice Coefficient: {dice_mean:.4f}\n\n")
            
            # æ‚£è€…è¡¨ç°æ’å
            f.write("PATIENT PERFORMANCE RANKING (by Dice Score)\n")
            f.write("-"*50 + "\n")
            
            # æŒ‰Diceå¾—åˆ†æ’åº
            sorted_patients = sorted(all_metrics, key=lambda x: x.get('dice', 0), reverse=True)
            
            f.write(f"{'Rank':<6} {'Patient':<10} {'Dice':<8} {'IoU':<8} {'Precision':<10} {'Recall':<8}\n")
            f.write("-"*60 + "\n")
            
            for i, patient in enumerate(sorted_patients[:10], 1):  # Top 10
                dice = patient.get('dice', 0)
                iou = patient.get('iou', 0)
                precision = patient.get('precision', 0)
                recall = patient.get('recall', 0)
                f.write(f"{i:<6} {patient['patient_id']:<10} {dice:.4f}   {iou:.4f}   "
                       f"{precision:.4f}     {recall:.4f}\n")
            
            f.write("\n")
            
            # é—®é¢˜åˆ†æ
            f.write("PERFORMANCE ANALYSIS\n")
            f.write("-"*20 + "\n")
            
            # æ‰¾å‡ºè¡¨ç°å·®çš„æ‚£è€…
            poor_performers = [p for p in all_metrics if p.get('dice', 0) < 0.5]
            if poor_performers:
                f.write(f"Patients with low performance (Dice < 0.5): {len(poor_performers)}\n")
                for p in poor_performers:
                    f.write(f"  - Patient {p['patient_id']}: Dice = {p.get('dice', 0):.4f}\n")
                f.write("\n")
            
            # ä½“ç§¯åˆ†æ
            total_true_volume = sum(p.get('true_tumor_volume', 0) for p in all_metrics)
            total_pred_volume = sum(p.get('pred_tumor_volume', 0) for p in all_metrics)
            
            f.write(f"VOLUME ANALYSIS:\n")
            f.write(f"Total true tumor volume: {total_true_volume:,} voxels\n")
            f.write(f"Total predicted volume: {total_pred_volume:,} voxels\n")
            f.write(f"Overall volume ratio: {total_pred_volume/total_true_volume:.4f}\n")
            
            if total_pred_volume > total_true_volume * 1.2:
                f.write("âš ï¸  Model tends to OVER-segment (predicts too much tumor)\n")
            elif total_pred_volume < total_true_volume * 0.8:
                f.write("âš ï¸  Model tends to UNDER-segment (misses tumor regions)\n")
            else:
                f.write("âœ… Model shows good volume estimation\n")
            
            f.write("\n")
        
        # è¯¦ç»†æ‚£è€…ç»“æœ
        f.write("DETAILED PATIENT RESULTS\n")
        f.write("-"*30 + "\n")
        
        for result in results:
            status = "âœ… SUCCESS" if result['success'] else "âŒ FAILED"
            f.write(f"Patient {result['patient_id']}: {status} ({result['duration']:.2f}s)\n")
            
            if result['success'] and all_metrics:
                # æ‰¾åˆ°å¯¹åº”çš„æŒ‡æ ‡
                patient_metrics = next((m for m in all_metrics if m['patient_id'] == result['patient_id']), None)
                if patient_metrics:
                    f.write(f"  Dice: {patient_metrics.get('dice', 'N/A'):.4f}, "
                           f"IoU: {patient_metrics.get('iou', 'N/A'):.4f}, "
                           f"Precision: {patient_metrics.get('precision', 'N/A'):.4f}\n")
            elif not result['success']:
                f.write(f"  Error: {result['output'][:100]}...\n")
        
        # æ¨èå’Œå»ºè®®
        f.write("\nRECOMMENDations\n")
        f.write("-"*15 + "\n")
        
        if all_metrics:
            dice_mean = metric_stats.get('dice', {}).get('mean', 0)
            dice_std = metric_stats.get('dice', {}).get('std', 0)
            
            if dice_mean < 0.7:
                f.write("ğŸ”§ Model performance is below clinical standards. Consider:\n")
                f.write("   - Collecting more training data\n")
                f.write("   - Data augmentation techniques\n")
                f.write("   - Hyperparameter tuning\n")
                f.write("   - Different loss functions (e.g., Focal Loss)\n")
            
            if dice_std > 0.2:
                f.write("ğŸ“Š High variability in performance across patients. Consider:\n")
                f.write("   - Analyzing patient characteristics\n")
                f.write("   - Stratified training approach\n")
                f.write("   - Ensemble methods\n")
            
            if len(poor_performers) > len(all_metrics) * 0.2:
                f.write("âš ï¸  More than 20% of patients show poor performance. Consider:\n")
                f.write("   - Reviewing data quality\n")
                f.write("   - Cross-validation analysis\n")
                f.write("   - Model architecture improvements\n")
        
        if failed > 0:
            f.write(f"ğŸš¨ {failed} tests failed. Check error logs and:\n")
            f.write("   - Verify data file integrity\n")
            f.write("   - Check system resources\n")
            f.write("   - Review error messages\n")

    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    generate_visualization_plots(all_metrics, metric_stats, output_base_dir)
    
    # ä¿å­˜æŒ‡æ ‡åˆ°JSON
    metrics_json_file = os.path.join(output_base_dir, "all_metrics.json")
    with open(metrics_json_file, 'w') as f:
        json.dump({
            'summary_stats': metric_stats if all_metrics else {},
            'patient_metrics': all_metrics,
            'execution_results': results
        }, f, indent=2)
    
    print(f"\n{'='*80}")
    print(f"COMPREHENSIVE REPORT GENERATED")
    print(f"{'='*80}")
    print(f"ğŸ“„ Detailed report: {report_file}")
    print(f"ğŸ“Š Metrics data: {metrics_json_file}")
    print(f"ğŸ“ˆ Visualization plots: {output_base_dir}/visualizations/")
    
    if all_metrics:
        dice_mean = metric_stats.get('dice', {}).get('mean', 0)
        print(f"\nğŸ¯ KEY RESULTS:")
        print(f"   Average Dice Score: {dice_mean:.4f}")
        print(f"   Patients tested: {len(all_metrics)}")
        print(f"   Success rate: {successful/total_patients*100:.1f}%")

def generate_visualization_plots(all_metrics, metric_stats, output_base_dir):
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
    if not all_metrics:
        return
    
    viz_dir = os.path.join(output_base_dir, "visualizations")
    os.makedirs(viz_dir, exist_ok=True)
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # 1. ä¸»è¦æŒ‡æ ‡åˆ†å¸ƒç›´æ–¹å›¾
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('Model Performance Metrics Distribution', fontsize=16)
    
    metrics_to_plot = ['dice', 'iou', 'accuracy', 'precision', 'recall', 'f1_score']
    
    for i, metric in enumerate(metrics_to_plot):
        row, col = i // 3, i % 3
        if metric in metric_stats:
            values = metric_stats[metric]['values']
            axes[row, col].hist(values, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
            axes[row, col].axvline(metric_stats[metric]['mean'], color='red', linestyle='--', 
                                 label=f'Mean: {metric_stats[metric]["mean"]:.3f}')
            axes[row, col].set_title(f'{metric.upper()} Distribution')
            axes[row, col].set_xlabel(metric.upper())
            axes[row, col].set_ylabel('Frequency')
            axes[row, col].legend()
            axes[row, col].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(viz_dir, 'metrics_distribution.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. æ‚£è€…æ€§èƒ½æ’åå›¾
    if 'dice' in metric_stats:
        sorted_patients = sorted(all_metrics, key=lambda x: x.get('dice', 0), reverse=True)
        
        plt.figure(figsize=(12, 8))
        patient_ids = [p['patient_id'] for p in sorted_patients]
        dice_scores = [p.get('dice', 0) for p in sorted_patients]
        
        bars = plt.bar(range(len(patient_ids)), dice_scores, color='lightcoral', alpha=0.7)
        plt.xlabel('Patient ID')
        plt.ylabel('Dice Score')
        plt.title('Patient Performance Ranking (Dice Score)')
        plt.xticks(range(len(patient_ids)), patient_ids, rotation=45)
        
        # æ·»åŠ å¹³å‡çº¿
        mean_dice = metric_stats['dice']['mean']
        plt.axhline(mean_dice, color='blue', linestyle='--', label=f'Average: {mean_dice:.3f}')
        
        # æ ‡è®°è¡¨ç°å¥½å’Œå·®çš„æ‚£è€…
        for i, (bar, score) in enumerate(zip(bars, dice_scores)):
            if score >= 0.8:
                bar.set_color('green')
            elif score <= 0.5:
                bar.set_color('red')
        
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(viz_dir, 'patient_ranking.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    # 3. æŒ‡æ ‡ç›¸å…³æ€§çƒ­å›¾
    import pandas as pd
    
    df_metrics = pd.DataFrame(all_metrics)
    numeric_cols = ['dice', 'iou', 'accuracy', 'precision', 'recall', 'f1_score', 'specificity']
    numeric_cols = [col for col in numeric_cols if col in df_metrics.columns]
    
    if len(numeric_cols) > 1:
        correlation_matrix = df_metrics[numeric_cols].corr()
        
        plt.figure(figsize=(10, 8))
        im = plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)
        plt.colorbar(im)
        
        # æ·»åŠ æ ‡ç­¾
        plt.xticks(range(len(numeric_cols)), [col.upper() for col in numeric_cols], rotation=45)
        plt.yticks(range(len(numeric_cols)), [col.upper() for col in numeric_cols])
        
        # æ·»åŠ æ•°å€¼
        for i in range(len(numeric_cols)):
            for j in range(len(numeric_cols)):
                plt.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', 
                        ha='center', va='center', color='black' if abs(correlation_matrix.iloc[i, j]) < 0.5 else 'white')
        
        plt.title('Metrics Correlation Heatmap')
        plt.tight_layout()
        plt.savefig(os.path.join(viz_dir, 'correlation_heatmap.png'), dpi=300, bbox_inches='tight')
        plt.close()

def generate_summary_report(results, output_base_dir):
    """ç”Ÿæˆç®€å•æ€»ç»“æŠ¥å‘Šï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    summary_file = os.path.join(output_base_dir, "test_summary.txt")
    
    total_patients = len(results)
    successful = sum(1 for r in results if r['success'])
    failed = total_patients - successful
    total_time = sum(r['duration'] for r in results)
    
    print(f"\n{'='*60}")
    print(f"TESTING SUMMARY")
    print(f"{'='*60}")
    print(f"Total patients tested: {total_patients}")
    print(f"Successful: {successful}")
    print(f"Failed: {failed}")
    print(f"Success rate: {successful/total_patients*100:.1f}%")
    print(f"Total time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)")
    print(f"Average time per patient: {total_time/total_patients:.2f} seconds")
    
    # å†™å…¥æ–‡ä»¶
    with open(summary_file, 'w') as f:
        f.write(f"BRAIN TUMOR SEGMENTATION - BATCH TEST SUMMARY\n")
        f.write(f"{'='*60}\n")
        f.write(f"Test completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write(f"OVERALL STATISTICS:\n")
        f.write(f"Total patients tested: {total_patients}\n")
        f.write(f"Successful: {successful}\n")
        f.write(f"Failed: {failed}\n")
        f.write(f"Success rate: {successful/total_patients*100:.1f}%\n")
        f.write(f"Total time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)\n")
        f.write(f"Average time per patient: {total_time/total_patients:.2f} seconds\n\n")
        
        f.write(f"DETAILED RESULTS:\n")
        f.write(f"{'-'*60}\n")
        for result in results:
            status = "âœ… SUCCESS" if result['success'] else "âŒ FAILED"
            f.write(f"Patient {result['patient_id']}: {status} ({result['duration']:.2f}s)\n")
            if not result['success']:
                f.write(f"  Error: {result['output'][:100]}...\n")
        
        if failed > 0:
            f.write(f"\nFAILED PATIENTS:\n")
            f.write(f"{'-'*30}\n")
            for result in results:
                if not result['success']:
                    f.write(f"Patient {result['patient_id']}: {result['output']}\n")
    
    print(f"\nSummary report saved to: {summary_file}")

def main():
    parser = argparse.ArgumentParser(description='Run comparison tests on all patients')
    parser.add_argument('--dataset_dir', default='dataset_segmentation',
                       help='Path to dataset directory (default: dataset_segmentation)')
    parser.add_argument('--model_path', default='best_brain_tumor_model.pth',
                       help='Path to trained model (default: best_brain_tumor_model.pth)')
    parser.add_argument('--output_dir', default='batch_test_results',
                       help='Output directory for all results (default: batch_test_results)')
    parser.add_argument('--compare_script', default='compare.py',
                       help='Path to compare script (default: compare.py)')
    parser.add_argument('--patients', nargs='*', default=None,
                       help='Specific patient IDs to test (default: all patients)')
    parser.add_argument('--skip_existing', action='store_true',
                       help='Skip patients that already have results')
    
    args = parser.parse_args()
    
    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    dataset_dir = os.path.abspath(args.dataset_dir)
    model_path = os.path.abspath(args.model_path)
    output_base_dir = os.path.abspath(args.output_dir)
    compare_script_path = os.path.abspath(args.compare_script)
    
    print(f"Dataset directory: {dataset_dir}")
    print(f"Model path: {model_path}")
    print(f"Output directory: {output_base_dir}")
    print(f"Compare script: {compare_script_path}")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    if not os.path.exists(dataset_dir):
        print(f"Error: Dataset directory not found: {dataset_dir}")
        return
    
    if not os.path.exists(model_path):
        print(f"Error: Model file not found: {model_path}")
        return
    
    if not os.path.exists(compare_script_path):
        print(f"Error: Compare script not found: {compare_script_path}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_base_dir, exist_ok=True)
    
    # æ‰¾åˆ°æ‰€æœ‰patients
    all_patients = find_all_patients(dataset_dir)
    
    if not all_patients:
        print("No patients found!")
        return
    
    # è¿‡æ»¤ç‰¹å®špatientsï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.patients:
        all_patients = [p for p in all_patients if p['patient_id'] in args.patients]
        print(f"Testing only specified patients: {args.patients}")
    
    # è·³è¿‡å·²å­˜åœ¨çš„ç»“æœï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.skip_existing:
        filtered_patients = []
        for patient in all_patients:
            patient_output_dir = os.path.join(output_base_dir, f"patient_{patient['patient_id']}")
            if os.path.exists(patient_output_dir) and os.listdir(patient_output_dir):
                print(f"Skipping patient {patient['patient_id']} (results already exist)")
            else:
                filtered_patients.append(patient)
        all_patients = filtered_patients
    
    if not all_patients:
        print("No patients to test!")
        return
    
    print(f"\nStarting batch testing of {len(all_patients)} patients...")
    
    # è¿è¡Œæµ‹è¯•
    results = []
    
    for i, patient_info in enumerate(all_patients):
        print(f"\nProgress: {i+1}/{len(all_patients)}")
        
        success, duration, output = run_single_test(
            patient_info, model_path, output_base_dir, compare_script_path
        )
        
        results.append({
            'patient_id': patient_info['patient_id'],
            'success': success,
            'duration': duration,
            'output': output
        })
        
        # æ¯å®Œæˆ5ä¸ªpatientæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        if (i + 1) % 5 == 0:
            successful_so_far = sum(1 for r in results if r['success'])
            print(f"\nProgress update: {i+1}/{len(all_patients)} completed, {successful_so_far} successful")
    
    # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
    print("\nCollecting and analyzing metrics...")
    all_metrics = collect_all_metrics(results, output_base_dir)
    
    # ç”Ÿæˆç®€å•æ€»ç»“æŠ¥å‘Šï¼ˆå‘åå…¼å®¹ï¼‰
    generate_summary_report(results, output_base_dir)
    
    # ç”Ÿæˆå…¨é¢æŠ¥å‘Š
    generate_comprehensive_report(results, all_metrics, output_base_dir)
    
    print(f"\nBatch testing completed!")
    print(f"Results saved in: {output_base_dir}")

if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    import sys
    if len(sys.argv) == 1:
        print("Brain Tumor Segmentation - Batch Testing Script")
        print("=" * 50)
        print("\nUsage examples:")
        print("python run_all_tests.py")
        print("python run_all_tests.py --patients 001 002 003")
        print("python run_all_tests.py --skip_existing")
        print("python run_all_tests.py --output_dir my_batch_results")
        print("\nThis script will:")
        print("- Find all patients in the dataset")
        print("- Run compare.py for each patient")
        print("- Generate individual result folders")
        print("- Create comprehensive analysis reports with visualizations")
        print("- Provide performance statistics and recommendations")
        print(f"\nCurrent directory: {CURRENT_DIR}")
    else:
        main()