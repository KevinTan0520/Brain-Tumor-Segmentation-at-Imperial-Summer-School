import os
import glob
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import nibabel as nib
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
from skimage import measure, morphology
from scipy import ndimage
import warnings
warnings.filterwarnings('ignore')

# è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•å¹¶è®¾ç½®ä¸ºå·¥ä½œç›®å½•
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
os.chdir(CURRENT_DIR)
print(f"Current working directory set to: {CURRENT_DIR}")

# ä» new_train.py å¯¼å…¥æ¨¡å‹æ¶æ„ç±»
class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        attention = torch.cat([avg_out, max_out], dim=1)
        attention = self.conv1(attention)
        return x * self.sigmoid(attention)

class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        
        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return x * self.sigmoid(out)

class SelfAttention(nn.Module):
    """ç®€åŒ–çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, in_dim, num_heads=8):
        super(SelfAttention, self).__init__()
        self.num_heads = num_heads
        self.head_dim = in_dim // num_heads
        self.scale = self.head_dim ** -0.5
        
        self.qkv = nn.Linear(in_dim, in_dim * 3, bias=False)
        self.proj = nn.Linear(in_dim, in_dim)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = F.softmax(attn, dim=-1)
        attn = self.dropout(attn)
        
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        return x

class TransformerBlock(nn.Module):
    """ç®€åŒ–çš„Transformerå—"""
    def __init__(self, dim, num_heads=8, mlp_ratio=4.0, dropout=0.1):
        super(TransformerBlock, self).__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = SelfAttention(dim, num_heads)
        self.norm2 = nn.LayerNorm(dim)
        
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_hidden_dim, dim),
            nn.Dropout(dropout),
        )
        
    def forward(self, x, H, W):
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x

class TransXAI_UNet(nn.Module):
    """
    åŸºäºTransXAIè®¾è®¡çš„æ··åˆCNN-Transformeræ¶æ„ - æ¨ç†ç‰ˆæœ¬
    """
    def __init__(self, in_channels=1, out_channels=1, features=[24, 48, 96, 192]):
        super(TransXAI_UNet, self).__init__()
        
        # CNNç‰¹å¾æå–å™¨
        self.initial_conv = nn.Sequential(
            nn.Conv2d(in_channels, features[0], 3, padding=1),
            nn.BatchNorm2d(features[0]),
            nn.ReLU(inplace=True),
            nn.Conv2d(features[0], features[0], 3, padding=1),
            nn.BatchNorm2d(features[0]),
            nn.ReLU(inplace=True)
        )
        
        # ä¸‹é‡‡æ ·è·¯å¾„
        self.downs = nn.ModuleList()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        for i, feature in enumerate(features):
            if i == 0:
                continue
            self.downs.append(self.double_conv(features[i-1], feature))
        
        # ç“¶é¢ˆå±‚
        self.bottleneck = self.double_conv(features[-1], features[-1]*2)
        
        # Transformerç¼–ç å™¨å±‚
        self.transformer_layers = nn.ModuleList([
            TransformerBlock(features[-1]*2, num_heads=8) for _ in range(4)
        ])
        
        # ä½ç½®ç¼–ç 
        self.pos_embed_conv = nn.Conv2d(features[-1]*2, features[-1]*2, 1)
        
        # ä¸Šé‡‡æ ·è·¯å¾„
        self.ups = nn.ModuleList()
        up_features = list(reversed(features))
        
        for i in range(len(features)):
            if i == 0:
                self.ups.append(nn.ConvTranspose2d(features[-1]*2, up_features[i], kernel_size=2, stride=2))
                self.ups.append(
                    nn.Sequential(
                        self.double_conv(up_features[i]*2, up_features[i]),
                        SpatialAttention(),
                        ChannelAttention(up_features[i])
                    )
                )
            else:
                self.ups.append(nn.ConvTranspose2d(up_features[i-1], up_features[i], kernel_size=2, stride=2))
                if i < len(features) - 1:
                    self.ups.append(
                        nn.Sequential(
                            self.double_conv(up_features[i]*2, up_features[i]),
                            SpatialAttention(),
                            ChannelAttention(up_features[i])
                        )
                    )
                else:
                    self.ups.append(self.double_conv(up_features[i]*2, up_features[i]))
        
        # æœ€ç»ˆåˆ†ç±»å±‚
        self.final_conv = nn.Sequential(
            nn.Conv2d(features[0], features[0]//2, 3, padding=1),
            nn.BatchNorm2d(features[0]//2),
            nn.ReLU(inplace=True),
            nn.Conv2d(features[0]//2, out_channels, 1)
        )
        
        # æ·±åº¦ç›‘ç£è¾“å‡º
        self.deep_supervision = nn.ModuleList([
            nn.Conv2d(up_features[i], out_channels, 1) for i in range(len(features)-1)
        ])
        
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def double_conv(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        # åˆå§‹å·ç§¯
        x = self.initial_conv(x)
        skip_connections = [x]
        
        # ä¸‹é‡‡æ ·è·¯å¾„
        for down in self.downs:
            x = self.pool(x)
            x = down(x)
            skip_connections.append(x)
        
        # ç“¶é¢ˆå±‚ + Transformerå¤„ç†
        x = self.pool(x)
        x = self.bottleneck(x)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embed_conv(x)
        
        # Transformerç¼–ç 
        B, C, H, W = x.shape
        x_flat = x.flatten(2).transpose(1, 2)
        
        for transformer in self.transformer_layers:
            x_flat = transformer(x_flat, H, W)
        
        x = x_flat.transpose(1, 2).reshape(B, C, H, W)
        
        skip_connections = skip_connections[::-1]
        
        # ä¸Šé‡‡æ ·è·¯å¾„
        for idx in range(0, len(self.ups), 2):
            x = self.ups[idx](x)
            skip_connection = skip_connections[idx//2]
            
            if x.shape != skip_connection.shape:
                x = F.interpolate(x, size=skip_connection.shape[2:], mode='bilinear', align_corners=False)
            
            concat_skip = torch.cat((skip_connection, x), dim=1)
            x = self.ups[idx+1](concat_skip)
        
        # æœ€ç»ˆè¾“å‡º - æ¨ç†æ—¶ç›´æ¥åº”ç”¨sigmoid
        final_output = self.final_conv(x)
        return torch.sigmoid(final_output)

def load_hybrid_model(model_path, device):
    """åŠ è½½è®­ç»ƒå¥½çš„æ··åˆæ¨¡å‹"""
    print(f"Loading hybrid model from: {model_path}")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    # æ£€æŸ¥æ¨¡å‹æ¶æ„ä¿¡æ¯
    if 'model_architecture' in checkpoint:
        print(f"   Model architecture: {checkpoint['model_architecture']}")
    
    if 'features' in checkpoint:
        features = checkpoint['features']
        print(f"   Model features: {features}")
    else:
        # é»˜è®¤ç‰¹å¾é…ç½®
        features = [24, 48, 96, 192]
        print(f"   Using default features: {features}")
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = TransXAI_UNet(in_channels=1, out_channels=1, features=features)
    
    # åŠ è½½æ¨¡å‹æƒé‡
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        print("âœ… Model loaded from checkpoint format")
        if 'best_dice' in checkpoint:
            print(f"   Model Best Dice: {checkpoint['best_dice']:.6f}")
        if 'epoch' in checkpoint:
            print(f"   Trained for {checkpoint['epoch']} epochs")
    else:
        model.load_state_dict(checkpoint)
        print("âœ… Model loaded from state dict format")
    
    model.to(device)
    model.eval()
    
    return model

def preprocess_slice(slice_data):
    """é¢„å¤„ç†å•ä¸ªåˆ‡ç‰‡æ•°æ®ï¼ˆä¸è®­ç»ƒä¿æŒä¸€è‡´ï¼‰"""
    if slice_data.max() <= slice_data.min() + 1e-6:
        return np.zeros_like(slice_data, dtype=np.float32)
    
    # åŸºäºä½“ç§¯çº§åˆ«çš„å½’ä¸€åŒ–ï¼ˆç®€åŒ–ç‰ˆï¼‰
    volume_mean = slice_data[slice_data > 0].mean() if slice_data[slice_data > 0].size > 0 else 0
    volume_std = slice_data[slice_data > 0].std() if slice_data[slice_data > 0].size > 0 else 1
    
    if volume_std > 1e-8:
        slice_data = (slice_data - volume_mean) / volume_std
    else:
        slice_data = slice_data - volume_mean
    
    # ç¨³å¥çš„å€¼åŸŸé™åˆ¶
    slice_data = np.clip(slice_data, -4, 4)
    
    return slice_data.astype(np.float32)

def process_single_case(fla_path, seg_path, model, device, output_base_dir):
    """å¤„ç†å•ä¸ªç—…ä¾‹ï¼ˆè®­ç»ƒé›†ä¸­çš„æ•°æ®ï¼‰"""
    print(f"\n{'='*80}")
    print(f"PROCESSING CASE: {fla_path}")
    print(f"{'='*80}")
    
    # è·å–æ‚£è€…ID
    patient_id = Path(fla_path).parent.name
    print(f"Patient ID: {patient_id}")
    
    # åˆ›å»ºæ‚£è€…è¾“å‡ºç›®å½•
    patient_output_dir = os.path.join(output_base_dir, patient_id)
    os.makedirs(patient_output_dir, exist_ok=True)
    
    # åŠ è½½NIfTIæ–‡ä»¶
    print("Loading NIfTI files...")
    fla_img = nib.load(fla_path)
    seg_img = nib.load(seg_path)
    
    fla_data = fla_img.get_fdata()
    seg_data = seg_img.get_fdata()
    
    print(f"   FLA image shape: {fla_data.shape}")
    print(f"   SEG image shape: {seg_data.shape}")
    print(f"   FLA value range: [{fla_data.min():.2f}, {fla_data.max():.2f}]")
    print(f"   Ground truth tumor voxels: {int(np.sum(seg_data > 0)):,}")
    
    # 3Dåˆ†å‰²é¢„æµ‹
    print("Running 3D segmentation...")
    depth = fla_data.shape[2]
    predictions = np.zeros_like(fla_data)
    
    # é€åˆ‡ç‰‡å¤„ç†
    for slice_idx in tqdm(range(depth), desc="Processing slices"):
        slice_data = fla_data[:, :, slice_idx].copy()
        
        # è·³è¿‡ç©ºåˆ‡ç‰‡
        if slice_data.max() <= slice_data.min() + 1e-6:
            continue
        
        # é¢„å¤„ç†
        processed_slice = preprocess_slice(slice_data)
        
        # è°ƒæ•´åˆ°240x240ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if processed_slice.shape != (240, 240):
            from skimage.transform import resize
            processed_slice = resize(processed_slice, (240, 240), preserve_range=True, anti_aliasing=True)
        
        # è½¬æ¢ä¸ºtensor
        input_tensor = torch.FloatTensor(processed_slice).unsqueeze(0).unsqueeze(0).to(device)
        
        # æ¨¡å‹é¢„æµ‹ï¼ˆsigmoidå·²åœ¨æ¨¡å‹å†…éƒ¨åº”ç”¨ï¼‰
        with torch.no_grad():
            pred = model(input_tensor)
            pred = pred.squeeze().cpu().numpy()
        
        # è°ƒæ•´å›åŸå§‹å°ºå¯¸ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if pred.shape != fla_data.shape[:2]:
            from skimage.transform import resize
            pred = resize(pred, fla_data.shape[:2], preserve_range=True, anti_aliasing=True)
        
        predictions[:, :, slice_idx] = pred
    
    # åå¤„ç†
    print("Applying post-processing...")
    
    # äºŒå€¼åŒ–
    binary_mask = (predictions > 0.5).astype(np.uint8)
    
    # 3Då½¢æ€å­¦é—­è¿ç®—
    selem = morphology.ball(1)
    binary_mask = morphology.binary_closing(binary_mask, selem)
    
    # è¿é€šç»„ä»¶åˆ†æ
    labeled_mask, num_components = measure.label(binary_mask, return_num=True)
    
    if num_components > 0:
        # ä¿ç•™å¤§ç»„ä»¶
        component_sizes = [(np.sum(labeled_mask == i), i) for i in range(1, num_components + 1)]
        final_mask = np.zeros_like(binary_mask)
        
        for size, label in sorted(component_sizes, reverse=True):
            if size >= 50:  # æœ€å°å°ºå¯¸é˜ˆå€¼
                final_mask[labeled_mask == label] = 1
    else:
        final_mask = binary_mask
    
    # 3Då­”æ´å¡«å……
    final_mask = ndimage.binary_fill_holes(final_mask)
    
    # ç»Ÿè®¡ä¿¡æ¯
    predicted_tumor_voxels = int(np.sum(final_mask))
    ground_truth_tumor_voxels = int(np.sum(seg_data > 0))
    
    print(f"   Predicted tumor voxels: {predicted_tumor_voxels:,}")
    print(f"   Ground truth tumor voxels: {ground_truth_tumor_voxels:,}")
    
    # ä¿å­˜é¢„æµ‹çš„åˆ†å‰²ç»“æœï¼ˆä¸calc.pyå…¼å®¹çš„æ ¼å¼ï¼‰
    pred_output_path = os.path.join(patient_output_dir, f"{patient_id}_seg.nii.gz")
    
    # åˆ›å»ºé¢„æµ‹NIfTIå›¾åƒ
    pred_img = nib.Nifti1Image(
        final_mask.astype(np.float32),
        fla_img.affine,
        fla_img.header
    )
    nib.save(pred_img, pred_output_path)
    print(f"   Predicted segmentation saved: {pred_output_path}")
    
    # å¤åˆ¶çœŸå®åˆ†å‰²åˆ°è¾“å‡ºç›®å½•ï¼ˆç”¨äºcalc.pyæ¯”è¾ƒï¼‰
    gt_output_path = os.path.join(patient_output_dir, f"{patient_id}_seg_gt.nii.gz")
    
    gt_img = nib.Nifti1Image(
        (seg_data > 0).astype(np.float32),
        seg_img.affine,
        seg_img.header
    )
    nib.save(gt_img, gt_output_path)
    print(f"   Ground truth segmentation saved: {gt_output_path}")
    
    # åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–
    print("Creating comparison visualization...")
    
    # æ‰¾åˆ°æœ‰è‚¿ç˜¤çš„åˆ‡ç‰‡
    gt_tumor_slices = []
    pred_tumor_slices = []
    
    for i in range(depth):
        if np.sum(seg_data[:, :, i] > 0) > 0:
            gt_tumor_slices.append(i)
        if np.sum(final_mask[:, :, i]) > 0:
            pred_tumor_slices.append(i)
    
    # é€‰æ‹©æœ‰ä»£è¡¨æ€§çš„åˆ‡ç‰‡è¿›è¡Œå¯è§†åŒ–
    all_tumor_slices = sorted(list(set(gt_tumor_slices + pred_tumor_slices)))
    
    if len(all_tumor_slices) > 0:
        # é€‰æ‹©æœ€å¤š8ä¸ªåˆ‡ç‰‡
        if len(all_tumor_slices) > 8:
            selected_indices = np.linspace(0, len(all_tumor_slices)-1, 8, dtype=int)
            selected_slices = [all_tumor_slices[i] for i in selected_indices]
        else:
            selected_slices = all_tumor_slices
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        axes = axes.flatten()
        
        for i, slice_idx in enumerate(selected_slices):
            if i >= 8:
                break
            
            # æ˜¾ç¤ºåŸå§‹å›¾åƒ
            img_slice = fla_data[:, :, slice_idx]
            axes[i].imshow(img_slice, cmap='gray', alpha=0.8)
            
            # å åŠ çœŸå®åˆ†å‰²ï¼ˆç»¿è‰²ï¼‰
            gt_slice = seg_data[:, :, slice_idx]
            if np.sum(gt_slice > 0) > 0:
                axes[i].contour(gt_slice, levels=[0.5], colors=['green'], linewidths=2, alpha=0.8)
            
            # å åŠ é¢„æµ‹åˆ†å‰²ï¼ˆçº¢è‰²ï¼‰
            pred_slice = final_mask[:, :, slice_idx]
            if np.sum(pred_slice) > 0:
                axes[i].contour(pred_slice, levels=[0.5], colors=['red'], linewidths=2, alpha=0.8)
            
            gt_count = int(np.sum(gt_slice > 0))
            pred_count = int(np.sum(pred_slice))
            
            axes[i].set_title(f'Slice {slice_idx}\nGT: {gt_count}, Pred: {pred_count}', fontsize=10)
            axes[i].axis('off')
        
        # éšè—æœªä½¿ç”¨çš„å­å›¾
        for i in range(len(selected_slices), 8):
            axes[i].axis('off')
        
        # æ·»åŠ å›¾ä¾‹
        from matplotlib.lines import Line2D
        legend_elements = [
            Line2D([0], [0], color='green', lw=2, label='Ground Truth'),
            Line2D([0], [0], color='red', lw=2, label='Prediction')
        ]
        fig.legend(handles=legend_elements, loc='upper right')
        
        plt.suptitle(f'Segmentation Comparison - {patient_id}\n'
                    f'GT: {ground_truth_tumor_voxels:,} voxels, Pred: {predicted_tumor_voxels:,} voxels', 
                    fontsize=16)
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        comparison_path = os.path.join(patient_output_dir, f"{patient_id}_comparison.png")
        plt.savefig(comparison_path, dpi=150, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"   Comparison visualization saved: {comparison_path}")
    
    print(f"âœ… Case {patient_id} completed")
    
    return {
        'patient_id': patient_id,
        'predicted_voxels': predicted_tumor_voxels,
        'ground_truth_voxels': ground_truth_tumor_voxels,
        'pred_path': pred_output_path,
        'gt_path': gt_output_path,
        'comparison_path': comparison_path if 'comparison_path' in locals() else None
    }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Batch Test Brain Tumor Segmentation on Training Set')
    parser.add_argument('--data_dir', default='dataset_segmentation/train',
                       help='Directory containing training data')
    parser.add_argument('--model_path', default='best_brain_tumor_model.pth',
                       help='Path to trained hybrid model')
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Processing device: {device}")
    
    # åŠ è½½æ··åˆæ¨¡å‹
    try:
        model = load_hybrid_model(args.model_path, device)
        print("Hybrid model loaded successfully\n")
    except Exception as e:
        print(f"Error loading model: {e}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    current_time = datetime.now()
    date_str = current_time.strftime('%Y%m%d')
    time_str = current_time.strftime('%H%M%S')
    output_base_dir = f"batch_tests_result_{date_str}_{time_str}"
    os.makedirs(output_base_dir, exist_ok=True)
    
    print(f"Output directory: {output_base_dir}")
    
    # æŸ¥æ‰¾è®­ç»ƒæ•°æ®æ–‡ä»¶
    if not os.path.exists(args.data_dir):
        print(f"Data directory not found: {args.data_dir}")
        return
    
    # æŸ¥æ‰¾æ‰€æœ‰ {id}/{id}_fla.nii.gz å’Œå¯¹åº”çš„ {id}_seg.nii.gz æ–‡ä»¶
    fla_pattern = os.path.join(args.data_dir, "*", "*_fla.nii.gz")
    fla_files = glob.glob(fla_pattern)
    
    if not fla_files:
        print(f"No FLA files found in {args.data_dir}")
        print("Expected structure: dataset_segmentation/train/{id}/{id}_fla.nii.gz")
        return
    
    # éªŒè¯å¯¹åº”çš„segæ–‡ä»¶å­˜åœ¨
    valid_cases = []
    for fla_path in fla_files:
        seg_path = fla_path.replace('_fla.nii.gz', '_seg.nii.gz')
        if os.path.exists(seg_path):
            valid_cases.append((fla_path, seg_path))
        else:
            patient_id = Path(fla_path).parent.name
            print(f"Warning: Missing seg file for {patient_id}")
    
    if not valid_cases:
        print("No valid case pairs found!")
        return
    
    print(f"Found {len(valid_cases)} valid cases:")
    for i, (fla_path, seg_path) in enumerate(valid_cases, 1):
        patient_id = Path(fla_path).parent.name
        print(f"  {i}. {patient_id}")
    
    # å¤„ç†æ‰€æœ‰ç—…ä¾‹
    print(f"\nStarting batch testing...")
    
    results = []
    successful = 0
    failed = 0
    
    for i, (fla_path, seg_path) in enumerate(valid_cases, 1):
        try:
            patient_id = Path(fla_path).parent.name
            print(f"\n[{i}/{len(valid_cases)}] Processing {patient_id}...")
            
            result = process_single_case(fla_path, seg_path, model, device, output_base_dir)
            results.append(result)
            successful += 1
            
        except Exception as e:
            patient_id = Path(fla_path).parent.name
            print(f"Failed to process {patient_id}: {e}")
            failed += 1
    
    # åˆ›å»ºç»“æœæ±‡æ€»
    print("\nCreating summary report...")
    
    summary_path = os.path.join(output_base_dir, "batch_test_summary.txt")
    with open(summary_path, 'w') as f:
        f.write("BATCH TEST SUMMARY REPORT\n")
        f.write("=" * 50 + "\n")
        f.write(f"Test Date: {current_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Model: {args.model_path}\n")
        f.write(f"Data Directory: {args.data_dir}\n")
        f.write(f"Output Directory: {output_base_dir}\n\n")
        
        f.write("PROCESSING RESULTS:\n")
        f.write("-" * 20 + "\n")
        f.write(f"Total cases: {len(valid_cases)}\n")
        f.write(f"Successful: {successful}\n")
        f.write(f"Failed: {failed}\n\n")
        
        if results:
            f.write("INDIVIDUAL CASE RESULTS:\n")
            f.write("-" * 30 + "\n")
            f.write(f"{'Patient ID':<15} {'GT Voxels':<12} {'Pred Voxels':<12} {'Ratio':<8}\n")
            f.write("-" * 50 + "\n")
            
            total_gt_voxels = 0
            total_pred_voxels = 0
            
            for result in results:
                gt_voxels = result['ground_truth_voxels']
                pred_voxels = result['predicted_voxels']
                ratio = pred_voxels / gt_voxels if gt_voxels > 0 else float('inf')
                
                f.write(f"{result['patient_id']:<15} {gt_voxels:<12,} {pred_voxels:<12,} {ratio:<8.3f}\n")
                
                total_gt_voxels += gt_voxels
                total_pred_voxels += pred_voxels
            
            f.write("-" * 50 + "\n")
            f.write(f"{'TOTAL':<15} {total_gt_voxels:<12,} {total_pred_voxels:<12,} "
                   f"{total_pred_voxels/total_gt_voxels if total_gt_voxels > 0 else float('inf'):<8.3f}\n")
        
        f.write("\nFILE STRUCTURE FOR calc.py:\n")
        f.write("-" * 25 + "\n")
        f.write("Each patient directory contains:\n")
        f.write("  - {patient_id}_seg.nii.gz      (predicted segmentation)\n")
        f.write("  - {patient_id}_seg_gt.nii.gz   (ground truth segmentation)\n")
        f.write("  - {patient_id}_comparison.png  (visual comparison)\n\n")
        f.write("To calculate metrics with calc.py:\n")
        f.write(f"python calc.py --pred_dir {output_base_dir} --gt_dir {output_base_dir}\n")
    
    # æœ€ç»ˆæ€»ç»“
    print(f"\n{'='*80}")
    print("BATCH TESTING COMPLETED")
    print(f"{'='*80}")
    print(f"Total cases: {len(valid_cases)}")
    print(f"Successful: {successful}")
    print(f"Failed: {failed}")
    print(f"Output directory: {output_base_dir}")
    print(f"Summary report: {summary_path}")
    print("\nğŸ“Š Results format compatible with calc.py:")
    print(f"   Predicted: {{patient_id}}_seg.nii.gz")
    print(f"   Ground truth: {{patient_id}}_seg_gt.nii.gz")
    print(f"\nğŸ§® To calculate all metrics:")
    print(f"python calc.py --pred_dir {output_base_dir} --gt_dir {output_base_dir}")

if __name__ == "__main__":
    main()